"""Deterministic benchmark over the curated Chinook subset.

The goal is to provide an end-to-end benchmark that mirrors how we would
evaluate against Spider while remaining lightweight enough to run offline (for
CI, demos, or resume review).  It emits:

* JSONL with per-query details
* JSON summary with accuracy + latency aggregates
* Compact CSV for quick inspection
* Optional PNG bar chart visualising latency & execution accuracy

Usage (local heuristic LLM, default):

    python -m benchmarks.benchmark_chinook

With a real OpenAI model (requires OPENAI_API_KEY):

    python -m benchmarks.benchmark_chinook --provider openai --model gpt-4o-mini
"""

from __future__ import annotations

import argparse
import csv
import json
import math
import shutil
import sqlite3
import statistics
import time
import xml.etree.ElementTree as ET
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

import sqlglot

from adapters.db.sqlite_adapter import SQLiteAdapter
from adapters.llm.openai_provider import OpenAIProvider
from nl2sql.ambiguity_detector import AmbiguityDetector
from nl2sql.executor import Executor
from nl2sql.generator import Generator
from nl2sql.pipeline import FinalResult, Pipeline
from nl2sql.planner import Planner
from nl2sql.repair import Repair
from nl2sql.safety import Safety
from nl2sql.verifier import Verifier

from benchmarks.chinook_subset import (
    CHINOOK_DATASET,
    DEFAULT_DB_PATH,
    SCHEMA_PREVIEW,
    ensure_chinook_subset_db,
)


# ---------------------------------------------------------------------------
# Local deterministic LLM implementation
# ---------------------------------------------------------------------------


def _normalize(text: str) -> str:
    return " ".join(text.lower().split())


class LocalChinookLLM:
    """Rule-based LLM that maps known questions to curated SQL answers."""

    provider_id = "benchmark-local"

    def __init__(self, dataset: Sequence[Dict[str, str]]) -> None:
        self._sql_by_question: Dict[str, str] = {
            _normalize(item["question"]): item["gold_sql"].strip()
            for item in dataset
        }

    # ---- Planner contract -------------------------------------------------
    def plan(self, *, user_query: str, schema_preview: str) -> Tuple[str, int, int, float]:
        plan = (
            "Understand the question, pick the relevant tables, "
            "join/aggregate if needed, and keep the query read-only."
        )
        return plan, 0, 0, 0.0

    # ---- Generator contract -----------------------------------------------
    def generate_sql(
        self,
        *,
        user_query: str,
        schema_preview: str,
        plan_text: str,
        clarify_answers: Optional[Any] = None,
    ) -> Tuple[str, str, int, int, float]:
        key = _normalize(user_query)
        if key not in self._sql_by_question:
            raise KeyError(f"Question not covered by benchmark dataset: {user_query}")

        sql = self._sql_by_question[key]
        rationale = "Retrieved deterministic template from Chinook benchmark map."
        return sql, rationale, 0, 0, 0.0

    # ---- Repair contract --------------------------------------------------
    def repair(
        self,
        *,
        sql: str,
        error_msg: str,
        schema_preview: str,
    ) -> Tuple[str, int, int, float]:
        # Deterministic repair: if the SQL exists in the curated map, return it;
        # otherwise fall back to the original SQL.
        normalized = sql.strip().lower()
        for candidate in self._sql_by_question.values():
            if candidate.strip().lower() == normalized:
                return candidate, 0, 0, 0.0
        return sql, 0, 0, 0.0


# ---------------------------------------------------------------------------
# Utility helpers
# ---------------------------------------------------------------------------


def _percentile(values: Sequence[float], pct: float) -> float:
    if not values:
        return 0.0
    pct = max(0.0, min(1.0, pct))
    ordered = sorted(values)
    k = (len(ordered) - 1) * pct
    f = math.floor(k)
    c = math.ceil(k)
    if f == c:
        return float(ordered[int(k)])
    d0 = ordered[f] * (c - k)
    d1 = ordered[c] * (k - f)
    return float(d0 + d1)


def _structural_match(sql_a: str, sql_b: str) -> bool:
    try:
        return sqlglot.parse_one(sql_a, read="sqlite") == sqlglot.parse_one(
            sql_b, read="sqlite"
        )
    except Exception:
        return False


def _exec_rows(conn: sqlite3.Connection, sql: str) -> List[Tuple[Any, ...]]:
    try:
        cur = conn.execute(sql)
        return [tuple(row) for row in cur.fetchall()]
    except Exception:
        return []


def _sum_cost(traces: Iterable[Dict[str, Any]]) -> float:
    total = 0.0
    for tr in traces:
        value = tr.get("cost_usd")
        if value is None:
            continue
        try:
            total += float(value)
        except (TypeError, ValueError):
            continue
    return total


# ---------------------------------------------------------------------------
# Core benchmark routine
# ---------------------------------------------------------------------------


@dataclass
class BenchmarkResult:
    id: str
    question: str
    sql_gold: str
    sql_pred: Optional[str]
    em: bool
    structural: bool
    exec_acc: bool
    latency_ms: float
    cost_usd: float
    ok: bool
    error: Optional[str]
    traces: List[Dict[str, Any]]


def build_pipeline(db_path: Path, provider: str) -> Pipeline:
    adapter = SQLiteAdapter(str(db_path))

    if provider == "openai":
        llm = OpenAIProvider()
    elif provider == "local":
        llm = LocalChinookLLM(CHINOOK_DATASET)
    else:
        raise ValueError(f"Unknown provider: {provider}")

    return Pipeline(
        detector=AmbiguityDetector(),
        planner=Planner(llm),
        generator=Generator(llm),
        safety=Safety(),
        executor=Executor(adapter),
        verifier=Verifier(),
        repair=Repair(llm),
    )


def run_benchmark(
    *,
    limit: Optional[int] = None,
    provider: str = "local",
    output_root: Path | None = None,
    sleep: float = 0.0,
) -> Dict[str, Any]:
    ensure_chinook_subset_db(DEFAULT_DB_PATH)
    db_path = DEFAULT_DB_PATH

    examples = CHINOOK_DATASET[: limit or None]

    pipeline = build_pipeline(db_path, provider)
    schema_preview = SCHEMA_PREVIEW

    conn = sqlite3.connect(db_path)

    results: List[BenchmarkResult] = []

    for ex in examples:
        qid = ex["id"]
        question = ex["question"]
        gold_sql = ex["gold_sql"].strip()

        t0 = time.perf_counter()
        error: Optional[str] = None
        sql_pred: Optional[str] = None
        ok = False
        traces: List[Dict[str, Any]] = []

        try:
            out: FinalResult = pipeline.run(
                user_query=question,
                schema_preview=schema_preview,
            )
            latency_ms = (time.perf_counter() - t0) * 1000.0
            traces = out.traces or []
            sql_pred = (out.sql or "").strip() if out.sql else None
            ok = bool(out.ok and not out.error and not out.ambiguous and sql_pred)
        except Exception as exc:
            latency_ms = (time.perf_counter() - t0) * 1000.0
            error = str(exc)

        if error is None and sql_pred is None:
            error = "Pipeline did not return SQL"

        em = False
        structural = False
        exec_acc = False
        cost_usd = _sum_cost(traces)

        if error is None and sql_pred:
            em = sql_pred.strip().lower() == gold_sql.strip().lower()
            structural = _structural_match(sql_pred, gold_sql)
            gold_rows = _exec_rows(conn, gold_sql)
            pred_rows = _exec_rows(conn, sql_pred)
            exec_acc = gold_rows == pred_rows
        else:
            ok = False

        results.append(
            BenchmarkResult(
                id=qid,
                question=question,
                sql_gold=gold_sql,
                sql_pred=sql_pred,
                em=em,
                structural=structural,
                exec_acc=exec_acc,
                latency_ms=latency_ms,
                cost_usd=cost_usd,
                ok=ok and exec_acc,
                error=error,
                traces=traces,
            )
        )

        if sleep:
            time.sleep(sleep)

    conn.close()

    output = _persist_results(results, provider=provider, output_root=output_root)
    return output


def _persist_results(
    results: Sequence[BenchmarkResult],
    *,
    provider: str,
    output_root: Path | None = None,
) -> Dict[str, Any]:
    output_root = output_root or Path("benchmarks") / "results" / "chinook"
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    run_dir = output_root / timestamp
    run_dir.mkdir(parents=True, exist_ok=True)

    jsonl_path = run_dir / "benchmark.jsonl"
    summary_path = run_dir / "summary.json"
    csv_path = run_dir / "summary.csv"
    chart_path = run_dir / "latency.svg"

    with jsonl_path.open("w", encoding="utf-8") as fh:
        for r in results:
            row = {
                "id": r.id,
                "question": r.question,
                "sql_gold": r.sql_gold,
                "sql_pred": r.sql_pred,
                "em": r.em,
                "structural_match": r.structural,
                "execution_accuracy": r.exec_acc,
                "latency_ms": round(r.latency_ms, 2),
                "cost_usd": round(r.cost_usd, 5),
                "ok": r.ok,
                "error": r.error,
                "traces": r.traces,
            }
            json.dump(row, fh, ensure_ascii=False)
            fh.write("\n")

    latencies = [r.latency_ms for r in results]
    exec_acc_rate = (
        sum(1 for r in results if r.exec_acc) / len(results) if results else 0.0
    )
    em_rate = sum(1 for r in results if r.em) / len(results) if results else 0.0
    structural_rate = (
        sum(1 for r in results if r.structural) / len(results) if results else 0.0
    )
    avg_latency = statistics.mean(latencies) if latencies else 0.0
    p95_latency = _percentile(latencies, 0.95)
    total_cost = sum(r.cost_usd for r in results)
    avg_cost = total_cost / len(results) if results else 0.0

    summary = {
        "dataset_size": len(results),
        "provider": provider,
        "timestamp": timestamp,
        "exec_accuracy": round(exec_acc_rate, 4),
        "exact_match": round(em_rate, 4),
        "structural_match": round(structural_rate, 4),
        "avg_latency_ms": round(avg_latency, 4),
        "p95_latency_ms": round(p95_latency, 4),
        "total_cost_usd": round(total_cost, 4),
        "avg_cost_usd": round(avg_cost, 4),
    }

    with summary_path.open("w", encoding="utf-8") as fh:
        json.dump(summary, fh, indent=2)
        fh.write("\n")

    with csv_path.open("w", newline="", encoding="utf-8") as fh:
        writer = csv.writer(fh)
        writer.writerow([
            "id",
            "question",
            "exec_acc",
            "exact_match",
            "structural_match",
            "latency_ms",
            "cost_usd",
            "error",
        ])
        for r in results:
            writer.writerow(
                [
                    r.id,
                    r.question,
                    "âœ…" if r.exec_acc else "âŒ",
                    "âœ…" if r.em else "âŒ",
                    "âœ…" if r.structural else "âŒ",
                    round(r.latency_ms, 2),
                    round(r.cost_usd, 5),
                    r.error or "",
                ]
            )

    chart_generated = _maybe_plot_latencies(results, chart_path)
    _update_latest_symlink(run_dir, output_root)

    print("\nðŸ“Š Summary:")
    print(json.dumps(summary, indent=2))
    print(
        "\nðŸ’¾ Artifacts written to:\n"
        f"- {jsonl_path}\n- {summary_path}\n- {csv_path}\n"
        f"- {chart_generated if chart_generated else 'chart skipped'}"
    )

    summary.update(
        {
            "jsonl_path": str(jsonl_path),
            "summary_path": str(summary_path),
            "csv_path": str(csv_path),
            "chart_path": str(chart_generated) if chart_generated else None,
        }
    )
    return summary


def _svg(elem: str, **attrs: Any) -> ET.Element:
    return ET.Element(elem, {k: str(v) for k, v in attrs.items()})


def _maybe_plot_latencies(
    results: Sequence[BenchmarkResult], chart_path: Path
) -> Optional[Path]:
    if not results:
        return None

    width, height = 720, 360
    margin = 60
    bar_gap = 12
    max_latency = max(r.latency_ms for r in results) or 1.0
    bar_area_width = width - 2 * margin
    bar_width = max(1, int((bar_area_width - bar_gap * (len(results) - 1)) / len(results)))

    svg = _svg(
        "svg",
        xmlns="http://www.w3.org/2000/svg",
        width=str(width),
        height=str(height),
        viewBox=f"0 0 {width} {height}",
    )

    svg.append(_svg("rect", x=0, y=0, width=width, height=height, fill="#ffffff"))

    # Axes
    svg.append(
        _svg(
            "line",
            x1=margin,
            y1=height - margin,
            x2=width - margin,
            y2=height - margin,
            stroke="#333",
            **{"stroke-width": 2},
        )
    )
    svg.append(
        _svg(
            "line",
            x1=margin,
            y1=margin,
            x2=margin,
            y2=height - margin,
            stroke="#333",
            **{"stroke-width": 2},
        )
    )

    # Title
    title = _svg(
        "text",
        x=width / 2,
        y=margin / 2,
        fill="#111",
        **{"font-size": 18, "text-anchor": "middle", "font-weight": "600"},
    )
    title.text = "Chinook Benchmark Latency"
    svg.append(title)

    for idx, result in enumerate(results):
        bar_height = (result.latency_ms / max_latency) * (height - 2 * margin)
        x = margin + idx * (bar_width + bar_gap)
        y = height - margin - bar_height
        color = "#2ca02c" if result.exec_acc else "#d62728"

        svg.append(
            _svg(
                "rect",
                x=f"{x:.1f}",
                y=f"{y:.1f}",
                width=bar_width,
                height=f"{bar_height:.1f}",
                rx=6,
                ry=6,
                fill=color,
            )
        )

        label = _svg(
            "text",
            x=x + bar_width / 2,
            y=height - margin + 24,
            fill="#333",
            **{"font-size": 14, "text-anchor": "middle"},
        )
        label.text = result.id
        svg.append(label)

        latency_txt = _svg(
            "text",
            x=x + bar_width / 2,
            y=y - 6,
            fill="#111",
            **{"font-size": 12, "text-anchor": "middle"},
        )
        latency_txt.text = f"{result.latency_ms:.1f} ms"
        svg.append(latency_txt)

        status_txt = _svg(
            "text",
            x=x + bar_width / 2,
            y=y - 24,
            fill=color,
            **{"font-size": 14, "text-anchor": "middle"},
        )
        status_txt.text = "âœ…" if result.exec_acc else "âŒ"
        svg.append(status_txt)

    # Y-axis labels (0 and max)
    for value in (0, max_latency):
        y = height - margin - (value / max_latency) * (height - 2 * margin)
        tick_txt = _svg(
            "text",
            x=margin - 10,
            y=y + 4,
            fill="#333",
            **{"font-size": 12, "text-anchor": "end"},
        )
        tick_txt.text = f"{value:.1f}"
        svg.append(tick_txt)

        svg.append(
            _svg(
                "line",
                x1=margin,
                y1=y,
                x2=width - margin,
                y2=y,
                stroke="#dddddd",
                **{"stroke-width": 1, "stroke-dasharray": "4 4"},
            )
        )

    chart_path.parent.mkdir(parents=True, exist_ok=True)
    ET.ElementTree(svg).write(chart_path, encoding="utf-8", xml_declaration=True)
    return chart_path


def _update_latest_symlink(run_dir: Path, output_root: Path) -> None:
    latest_dir = output_root / "latest"
    latest_dir.mkdir(parents=True, exist_ok=True)

    for item in run_dir.iterdir():
        target = latest_dir / item.name
        if target.exists():
            if target.is_file():
                target.unlink()
            elif target.is_dir():
                shutil.rmtree(target)
        shutil.copy2(item, target)


# ---------------------------------------------------------------------------
# CLI entry point
# ---------------------------------------------------------------------------


def main(argv: Optional[Sequence[str]] = None) -> None:
    parser = argparse.ArgumentParser(description="Run Chinook subset benchmark")
    parser.add_argument("--limit", type=int, default=None, help="Limit number of examples")
    parser.add_argument(
        "--provider",
        type=str,
        default="local",
        choices=["local", "openai"],
        help="LLM provider to use",
    )
    parser.add_argument(
        "--sleep",
        type=float,
        default=0.0,
        help="Optional delay between queries (seconds)",
    )
    parser.add_argument(
        "--output-root",
        type=str,
        default=None,
        help="Override root directory for artifacts",
    )

    args = parser.parse_args(argv)

    run_benchmark(
        limit=args.limit,
        provider=args.provider,
        output_root=Path(args.output_root) if args.output_root else None,
        sleep=args.sleep,
    )


if __name__ == "__main__":
    main()

